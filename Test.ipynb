{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "da4e1e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pesante/Study_Tools/anaconda3/envs/text2event/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1006: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    MBartTokenizer,\n",
    "    default_data_collator,\n",
    "    AutoModelWithLMHead,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "if tokenizer.encode(\"<extra_id_0> <extra_id_1>\") != [32099, 32098, 1]:\n",
    "    # For non-t5 tokenizer\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"<extra_id_0>\", \"<extra_id_1>\"]})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544672ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "def get_state_valid_tokens(src_sentence: List[str], tgt_generated: List[str]) -> List[str]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066a2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentence = [\"1\",\"2\",\"3\"]\n",
    "tgt_generated = [\"4\",\"5\",\"6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8d0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_state_valid_tokens(src_sentence, tgt_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6db990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3faf4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "la = {'32':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a972775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for _, sub_label_seq in la.items():\n",
    "    print(_)\n",
    "    print(sub_label_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f598f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token_set = {1, 0}\n",
    "tgt_generated = ['21','32d','3sc']\n",
    "a = list(filter(lambda x: x[1] in special_token_set, list(enumerate(tgt_generated))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8624484b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af4cc969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "327ec798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5610bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenizer.convert_tokens_to_ids('<extra_id_0>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fb90b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32099\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aae8a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(apple)', ',(new),', '(noway)']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dbc8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space(text):\n",
    "    \"\"\"\n",
    "    add space between special token\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_text_list = list()\n",
    "    for item in zip(split_bracket.findall(text), split_bracket.split(text)[1:]):\n",
    "        new_text_list += item\n",
    "    return ' '.join(new_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6e68a",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78dc742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-566c696f8d412887\n",
      "Reusing dataset json (/home/pesante/.cache/huggingface/datasets/json/default-566c696f8d412887/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "data_files = {}\n",
    "data_files[\"train\"] = './data/text2tree/dyiepp_ace2005_subtype/train.json'\n",
    "extension = 'json'\n",
    "datasets = load_dataset(extension, data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35de264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'event']\n"
     ]
    }
   ],
   "source": [
    "column_names = datasets[\"train\"].column_names\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "902b6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets[\"train\"]\n",
    "text_column = \"text\"\n",
    "summary_column = \"event\"\n",
    "inputs = train_dataset[text_column]\n",
    "targets = train_dataset[summary_column]\n",
    "padding = \"max_length\"\n",
    "model_inputs = tokenizer(inputs, max_length= 256, padding=padding, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af0368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd561b",
   "metadata": {},
   "source": [
    "# New data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bbf9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from data_convert.format.text2tree import Text2Tree\n",
    "from data_convert.task_format.event_extraction import Event, DyIEPP\n",
    "from data_convert.utils import read_file, check_output, data_counter_to_table, get_schema, output_schema\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data_class=DyIEPP\n",
    "target_class=Text2Tree\n",
    "type_format='subtype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71cf6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/923 [00:00<00:00, 37516.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "in_filename = \"data/raw_data/ace05-EN/dev.oneie.json\"\n",
    "output_filename = \"data/new_text2tree/ace2005_event/dev\"\n",
    "if not os.path.exists(output_filename):\n",
    "        os.makedirs(output_filename)\n",
    "event_output = open(output_filename + '.json', 'w')\n",
    "\n",
    "count = 0;\n",
    "for line in read_file(in_filename):\n",
    "    document = Event(json.loads(line.strip()))\n",
    "    if len(document.relations) > 1:\n",
    "        break\n",
    "#     for sentence in document.generate_sentence(type_format=type_format):\n",
    "#         print(sentence['events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46189ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event_type': 'Movement:Transport', 'id': 'CNN_CF_20030303.1900.02-5-EV0', 'trigger': {'start': 5, 'end': 6, 'text': 'deploy'}, 'arguments': [{'entity_id': 'CNN_CF_20030303.1900.02-5-E2', 'text': 'soldiers', 'role': 'Artifact'}, {'entity_id': 'CNN_CF_20030303.1900.02-5-E4', 'text': 'region', 'role': 'Destination'}]}\n"
     ]
    }
   ],
   "source": [
    "for event in (document.events):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8784edf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relation_type': 'PART-WHOLE', 'id': 'CNN_CF_20030303.1900.02-5-R0', 'arguments': [{'entity_id': 'CNN_CF_20030303.1900.02-5-E1', 'text': 'Army', 'role': 'Arg-1'}, {'entity_id': 'CNN_CF_20030303.1900.02-5-E0', 'text': 'U.S.', 'role': 'Arg-2'}]}\n",
      "{'relation_type': 'ORG-AFF', 'id': 'CNN_CF_20030303.1900.02-5-R1', 'arguments': [{'entity_id': 'CNN_CF_20030303.1900.02-5-E2', 'text': 'soldiers', 'role': 'Arg-1'}, {'entity_id': 'CNN_CF_20030303.1900.02-5-E1', 'text': 'Army', 'role': 'Arg-2'}]}\n",
      "{'relation_type': 'PHYS', 'id': 'CNN_CF_20030303.1900.02-5-R2', 'arguments': [{'entity_id': 'CNN_CF_20030303.1900.02-5-E2', 'text': 'soldiers', 'role': 'Arg-1'}, {'entity_id': 'CNN_CF_20030303.1900.02-5-E4', 'text': 'region', 'role': 'Arg-2'}]}\n",
      "{'relation_type': 'PART-WHOLE', 'id': 'CNN_CF_20030303.1900.02-5-R3', 'arguments': [{'entity_id': 'CNN_CF_20030303.1900.02-5-E3', 'text': 'Persian Gulf', 'role': 'Arg-1'}, {'entity_id': 'CNN_CF_20030303.1900.02-5-E4', 'text': 'region', 'role': 'Arg-2'}]}\n"
     ]
    }
   ],
   "source": [
    "for relation in (document.relations):\n",
    "    print(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a86229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relations(document):\n",
    "    relations = list()\n",
    "    for relation in document.relations:\n",
    "        arguments = list()\n",
    "        relation_type = relation['relation_type']\n",
    "        for argument in relation['arguments']:\n",
    "            argument_entity = document.entities[argument['entity_id']]\n",
    "            arguments += [list(range(argument_entity['start'], argument_entity['end']))]\n",
    "        for old_relation in relations:\n",
    "            if relation_type == old_relation['type']:\n",
    "                old_relation['arguments'].append(arguments)\n",
    "                continue\n",
    "        relations += [{'type': relation_type, 'arguments': [arguments]}]\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f9339fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'PART-WHOLE', 'arguments': [[[8], [7]], [[12, 13], [14]]]}, {'type': 'ORG-AFF', 'arguments': [[[9], [8]]]}, {'type': 'PHYS', 'arguments': [[[9], [14]]]}]\n"
     ]
    }
   ],
   "source": [
    "relations = generate_relations(document)\n",
    "print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9dd2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relation(document):\n",
    "    for relations_in_sentence, sentence_start in zip(document.relations, document.sentence_start):\n",
    "        relations = list()\n",
    "        type_set = set()\n",
    "        for relation in relations_in_sentence:\n",
    "#             'arguments': [['Arg-1', [9]], ['Arg-2', [14]]]\n",
    "            arguments = [list(range(relation[0]-sentence_start, relation[1]+1-sentence_start)),\n",
    "                         list(range(relation[2]-sentence_start,relation[3]+1-sentence_start))]\n",
    "            relation_type = relation[4].split('.')[0]\n",
    "            if relation_type in type_set:\n",
    "                for old_relation in relations:\n",
    "                    if relation_type == old_relation['type']:\n",
    "                        old_relation['arguments'].append(arguments)\n",
    "            else:\n",
    "                type_set.add(relation_type)\n",
    "                relations += [{'type': relation_type, 'arguments': [arguments]}]\n",
    "        print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "524e8880",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Event' object has no attribute 'sentence_start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44862/1193920764.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_relation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_44862/3006856016.py\u001b[0m in \u001b[0;36mgenerate_relation\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_relation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrelations_in_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtype_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrelation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrelations_in_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Event' object has no attribute 'sentence_start'"
     ]
    }
   ],
   "source": [
    "generate_relation(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2643b0b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Event' object has no attribute 'ner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_836619/1813248004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mName_Entity_Type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents_in_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents_in_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtrigger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Event' object has no attribute 'ner'"
     ]
    }
   ],
   "source": [
    "Name_Entity_Type = set()\n",
    "for ner, sentence, events_in_sentence, sentence_start in zip(document.ner , document.sentences, document.events, document.sentence_start):\n",
    "    events = list()\n",
    "    for event in events_in_sentence:\n",
    "        trigger, event_type = event[0]\n",
    "        trigger_ner = ner\n",
    "        trigger -= sentence_start\n",
    "\n",
    "        suptype, subtype = event_type.split('.')\n",
    "\n",
    "        if type_format == 'subtype':\n",
    "            event_type = subtype\n",
    "        elif type_format == 'suptype':\n",
    "            event_type = suptype\n",
    "        else:\n",
    "            event_type = suptype + type_format + subtype\n",
    "\n",
    "        arguments = list()\n",
    "        for start, end, role in event[1:]:\n",
    "            start -= sentence_start\n",
    "            end -= sentence_start\n",
    "            arguments += [[role, list(range(start, end + 1))]]\n",
    "\n",
    "        for argument in arguments:\n",
    "            for ner_pos in ner:\n",
    "                Name_Entity_Type.add(ner_pos[2])\n",
    "                if((ner_pos[0]-sentence_start) == argument[1][0]):\n",
    "                    argument.insert(1, ner_pos[2])\n",
    "            if(len(argument) != 3):\n",
    "                print(\"Wrong\")\n",
    "\n",
    "        event = {'type': event_type, 'tokens': [trigger], 'arguments': arguments}\n",
    "\n",
    "        events += [event]\n",
    "#         print(events)\n",
    "        if(len(event['arguments']) >1):\n",
    "            A_predict = {'tokens': sentence, 'events': events}\n",
    "# print(A_predict)\n",
    "print(Name_Entity_Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93312b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4674, 517, 1]\n",
      "[3, 8575, 566, 1]\n",
      "[9664, 188, 1]\n",
      "[301, 5618, 1]\n",
      "[377, 5173, 1]\n",
      "[3, 8742, 1]\n",
      "[350, 5668, 1]\n",
      "[32099, 1]\n"
     ]
    }
   ],
   "source": [
    "list_A = ['ORG', 'VEH', 'WEA', 'LOC', 'FAC', 'PER', 'GPE']\n",
    "for a in list_A:\n",
    "    entity_token = tokenizer.encode(a)\n",
    "    print(entity_token)\n",
    "print(tokenizer.encode('<extra_id_0>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e6adf2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32100, 32101, 32102, 32103, 32104, 8742, 32105]\n",
      "Wrong\n",
      "LOC\n",
      "0\n",
      "1.2132\n"
     ]
    }
   ],
   "source": [
    "entity_dic = {'ORG', 'VEH', 'WEA', 'LOC', 'FAC', 'PER', 'GPE'}\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": list_A})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(tokenizer.convert_tokens_to_ids(list_A))\n",
    "if 'ORG' in list_A:\n",
    "    print(\"Wrong\")\n",
    "    \n",
    "print(tokenizer.decode(32103))\n",
    "print(tokenizer.pad_token_id)\n",
    "a = 1.213232\n",
    "b = round(a, 4)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e4423",
   "metadata": {},
   "source": [
    "# OneIE data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00e04633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'Attack', 'tokens': [8], 'arguments': []}]\n",
      "[{'type': 'Attack', 'tokens': [8], 'arguments': []}, {'type': 'End-Position', 'tokens': [10], 'arguments': [['Person', 'PER', [12, 13]], ['Entity', 'GPE', [17]]]}]\n"
     ]
    }
   ],
   "source": [
    "events = list()\n",
    "# print(document.events)\n",
    "# print(document.entities)\n",
    "for event, entity in zip(document.events, document.entities):\n",
    "#     print(event)\n",
    "#     print(entity)\n",
    "    arguments = list()\n",
    "    for argument in event['arguments']:\n",
    "        argument_entity = document.entities[argument['entity_id']]\n",
    "#         print(\"argument_entity\", argument_entity)\n",
    "        arguments += [[argument['role'], argument_entity['entity_type'] ,list(range(argument_entity['start'], argument_entity['end']))]]\n",
    "\n",
    "    suptype, subtype = event['event_type'].split(':')\n",
    "\n",
    "    if type_format == 'subtype':\n",
    "        event_type = subtype\n",
    "    elif type_format == 'suptype':\n",
    "        event_type = suptype\n",
    "    else:\n",
    "        event_type = suptype + type_format + subtype\n",
    "\n",
    "    events += [{\n",
    "        'type': event_type,\n",
    "        'tokens': list(range(event['trigger']['start'], event['trigger']['end'])),\n",
    "        'arguments': arguments\n",
    "    }]\n",
    "    print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ner, sentence, events_in_sentence, sentence_start in zip(document.ner , document.sentences, document.events, document.sentence_start):\n",
    "        if(len(ner) > 0):\n",
    "            print((ner))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410da613",
   "metadata": {},
   "source": [
    "# Annotated from event text to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e92ed6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_convert.utils import read_file, check_output, data_counter_to_table, get_schema, output_schema\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "type_start = '<extra_id_0>'\n",
    "type_end = '<extra_id_1>'\n",
    "role_start = '<extra_id_2>'\n",
    "role_end = '<extra_id_3>'\n",
    "\n",
    "def get_str_from_tokens(tokens, sentence, separator=' '):\n",
    "    start, end_exclude = tokens[0], tokens[-1] + 1\n",
    "    return separator.join(sentence[start:end_exclude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56f1ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Attack', 'tokens': [7], 'arguments': [['Attacker', 'PER', [5]], ['Attacker', 'PER', [14]], ['Attacker', 'PER', [19]]]}\n",
      "fight\n",
      "{('Attack', 'Attacker')}\n"
     ]
    }
   ],
   "source": [
    "event_schema_set = set()\n",
    "\n",
    "for event in A_predict['events']:\n",
    "    print(event)\n",
    "    event_schema_set = event_schema_set | get_schema(event)\n",
    "    sep = ' '\n",
    "    predicate = sep.join([A_predict['tokens'][index]\n",
    "                          for index in event['tokens']])\n",
    "#     counter['pred'].update([predicate])\n",
    "#     counter['type'].update([event['type']])\n",
    "#     data_counter[in_filename].update(['event'])\n",
    "#     for argument in event['arguments']:\n",
    "#         data_counter[in_filename].update(['argument'])\n",
    "#         counter['role'].update([argument[0]])\n",
    "\n",
    "print(predicate)\n",
    "print(event_schema_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ad2a636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain has deployed some 45,000 troops to fight with the more than 250,000 US soldiers lined up against Iraqi troops .\n",
      " \n",
      "<extra_id_0> <extra_id_0> Attack fight <extra_id_0> Attacker PER troops <extra_id_1> <extra_id_0> Attacker PER soldiers <extra_id_1> <extra_id_0> Attacker PER troops <extra_id_1> <extra_id_1> <extra_id_1>\n"
     ]
    }
   ],
   "source": [
    "tokens=A_predict['tokens']\n",
    "predicate_arguments=A_predict['events']\n",
    "\n",
    "token_separator = ' '\n",
    "\n",
    "event_str_rep_list = list()\n",
    "\n",
    "for predicate_argument in predicate_arguments:\n",
    "    event_type = predicate_argument['type']\n",
    "\n",
    "    # predicate_argument['tokens'] is the trigger index\n",
    "    # tokens is the sentence tokens, we get the trigger text span here\n",
    "    predicate_text = get_str_from_tokens(predicate_argument['tokens'], tokens, separator=token_separator)\n",
    "\n",
    "    # prefix_tokens[predicate_argument['tokens'][0]] = ['[ ']\n",
    "    # suffix_tokens[predicate_argument['tokens'][-1]] = [' ]']\n",
    "\n",
    "    role_str_list = list()\n",
    "    # role_name is the argument role, role_tokens are corresponding text span index\n",
    "    for role_name, role_entity, role_tokens in predicate_argument['arguments']:\n",
    "        # if role_name == 'Place' or role_name.startswith('Time'):\n",
    "        if role_name == event_type:\n",
    "            continue\n",
    "        # get the role text span from role tokens index\n",
    "        role_text = get_str_from_tokens(role_tokens, tokens, separator=token_separator)\n",
    "#         print(role_text)\n",
    "#         print(role_entity)\n",
    "        if False:\n",
    "            role_str = ' '.join([role_start, role_name, role_entity ,role_text, role_end])\n",
    "        else:\n",
    "            role_str = ' '.join([type_start, role_name,role_entity ,role_text, type_end])\n",
    "        # All arguments in the sentence\n",
    "#         print(role_str)\n",
    "        role_str_list += [role_str]\n",
    "    role_str_list_str = ' '.join(role_str_list)\n",
    "    event_str_rep = f\"{type_start} {event_type} {predicate_text} {role_str_list_str} {type_end}\"\n",
    "    event_str_rep_list += [event_str_rep]\n",
    "\n",
    "# print(tokens)    \n",
    "source_text = token_separator.join(tokens)\n",
    "target_text = ' '.join(event_str_rep_list)\n",
    "\n",
    "if not False:\n",
    "    target_text = f'{type_start} ' + \\\n",
    "                    ' '.join(event_str_rep_list) + f' {type_end}'\n",
    "\n",
    "print(source_text) \n",
    "print(\" \")\n",
    "print(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db7691c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type_start  type_end', 'type_start  type_end']\n"
     ]
    }
   ],
   "source": [
    "a = [\"%s%s\" % (\"type_start  \", \"type_end\")] * 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61858c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ab2f49d",
   "metadata": {},
   "source": [
    "# Get Label Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "90a5a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_name_tree(label_name_list, tokenizer, end_symbol='<end>'):\n",
    "    # Change recurring into non-recurring labels, \n",
    "    sub_token_tree = dict()\n",
    "\n",
    "    # this is label_name token ids\n",
    "    label_tree = dict()\n",
    "    for typename in label_name_list:\n",
    "#         print(typename)\n",
    "        after_tokenized = tokenizer.encode(typename)\n",
    "        label_tree[typename] = after_tokenized\n",
    "    print(label_tree)\n",
    "    for _, sub_label_seq in label_tree.items():\n",
    "        # sub_label_seq is the tokenize_ids of typename\n",
    "        parent = sub_token_tree\n",
    "        for value in sub_label_seq:\n",
    "            if value not in parent:\n",
    "                parent[value] = dict()\n",
    "            parent = parent[value]\n",
    "        parent[end_symbol] = None\n",
    "\n",
    "    return sub_token_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2639a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Extradite': [8505, 10700, 1], 'Sentence': [4892, 17, 1433, 1], 'Trial-Hearing': [20660, 18, 3845, 9, 1007, 1], 'Die': [316, 1], 'Elect': [3, 21543, 1], 'Declare-Bankruptcy': [28596, 60, 18, 21347, 9433, 75, 63, 1], 'Appeal': [25024, 1], 'Attack': [24655, 1], 'Merge-Org': [4039, 397, 18, 7395, 122, 1], 'Charge-Indict': [15907, 18, 1570, 12194, 1], 'Release-Parole': [13048, 18, 13212, 32, 109, 1], 'Divorce': [2043, 1967, 565, 1], 'Pardon': [2180, 2029, 1], 'Sue': [17564, 1], 'Start-Position': [3273, 18, 345, 32, 7, 4749, 1], 'Meet': [12325, 1], 'Demonstrate': [15782, 29, 7, 17, 2206, 1], 'Execute': [25183, 15, 1], 'Convict': [1193, 7287, 17, 1], 'Transport': [7608, 1], 'Transfer-Ownership': [9900, 18, 667, 210, 687, 2009, 1], 'Arrest-Jail': [1533, 6216, 18, 683, 9, 173, 1], 'Start-Org': [3273, 18, 7395, 122, 1], 'Transfer-Money': [9900, 18, 9168, 15, 63, 1], 'Phone-Write': [8924, 18, 24965, 15, 1], 'End-Position': [3720, 18, 345, 32, 7, 4749, 1], 'Fine': [11456, 1], 'End-Org': [3720, 18, 7395, 122, 1], 'Acquit': [4292, 10073, 1], 'Nominate': [465, 51, 8660, 1], 'Be-Born': [493, 18, 279, 127, 29, 1], 'Injure': [86, 10609, 15, 1], 'Marry': [1571, 651, 1]}\n"
     ]
    }
   ],
   "source": [
    "from extraction.event_schema import EventSchema\n",
    "event_schema = './data/text2tree/dyiepp_ace2005_subtype/event.schema'\n",
    "decoding_type_schema = EventSchema.read_from_file(event_schema)\n",
    "# print(decoding_type_schema.type_list)\n",
    "# print(decoding_type_schema.role_list)\n",
    "# print(decoding_type_schema.type_role_dict)\n",
    "type_tree = get_label_name_tree(decoding_type_schema.type_list, tokenizer, end_symbol='<tree-end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2eb6d37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{210: {687: {2009: {1: {'<tree-end>': None}}}}}\n"
     ]
    }
   ],
   "source": [
    "# print(list(type_tree.keys()))\n",
    "# print(\" \")\n",
    "subtree = type_tree[9900][18][667]\n",
    "print(len(subtree))\n",
    "print(subtree)\n",
    "if '<tree-end>' in subtree:\n",
    "    print(\"end_tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0452bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Plaintiff': [30837, 1], 'Instrument': [13507, 1], 'Person': [5780, 1], 'Entity': [4443, 485, 1], 'Beneficiary': [30570, 1208, 1], 'Adjudicator': [1980, 14312, 447, 1016, 1], 'Seller': [19980, 1], 'Prosecutor': [749, 7, 15, 3044, 127, 1], 'Artifact': [1261, 23, 8717, 1], 'Vehicle': [15095, 1], 'Attacker': [24655, 49, 1], 'Origin': [19477, 1], 'Victim': [12060, 2998, 1], 'Defendant': [3, 16196, 989, 288, 1], 'Agent': [8628, 1], 'Org': [955, 122, 1], 'Buyer': [19099, 1], 'Place': [3399, 1], 'Destination': [19344, 257, 1], 'Target': [12615, 1], 'Recipient': [419, 3389, 4741, 1], 'Giver': [6434, 52, 1]}\n",
      "dict_keys([30837, 13507, 5780, 4443, 30570, 1980, 19980, 749, 1261, 15095, 24655, 19477, 12060, 3, 8628, 955, 19099, 3399, 19344, 12615, 419, 6434])\n",
      "{30837: {1: {'<tree-end>': None}}, 13507: {1: {'<tree-end>': None}}, 5780: {1: {'<tree-end>': None}}, 4443: {485: {1: {'<tree-end>': None}}}, 30570: {1208: {1: {'<tree-end>': None}}}, 1980: {14312: {447: {1016: {1: {'<tree-end>': None}}}}}, 19980: {1: {'<tree-end>': None}}, 749: {7: {15: {3044: {127: {1: {'<tree-end>': None}}}}}}, 1261: {23: {8717: {1: {'<tree-end>': None}}}}, 15095: {1: {'<tree-end>': None}}, 24655: {49: {1: {'<tree-end>': None}}}, 19477: {1: {'<tree-end>': None}}, 12060: {2998: {1: {'<tree-end>': None}}}, 3: {16196: {989: {288: {1: {'<tree-end>': None}}}}}, 8628: {1: {'<tree-end>': None}}, 955: {122: {1: {'<tree-end>': None}}}, 19099: {1: {'<tree-end>': None}}, 3399: {1: {'<tree-end>': None}}, 19344: {257: {1: {'<tree-end>': None}}}, 12615: {1: {'<tree-end>': None}}, 419: {3389: {4741: {1: {'<tree-end>': None}}}}, 6434: {52: {1: {'<tree-end>': None}}}}\n"
     ]
    }
   ],
   "source": [
    "role_tree = get_label_name_tree(decoding_type_schema.role_list, tokenizer, end_symbol='<tree-end>')\n",
    "print(role_tree.keys())\n",
    "print(role_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5221c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{30837: {1: {'<tree-end>': None}}, 13507: {1: {'<tree-end>': None}}, 5780: {1: {'<tree-end>': None}}, 4443: {485: {1: {'<tree-end>': None}}}, 30570: {1208: {1: {'<tree-end>': None}}}, 1980: {14312: {447: {1016: {1: {'<tree-end>': None}}}}}, 19980: {1: {'<tree-end>': None}}, 749: {7: {15: {3044: {127: {1: {'<tree-end>': None}}}}}}, 1261: {23: {8717: {1: {'<tree-end>': None}}}}, 15095: {1: {'<tree-end>': None}}, 24655: {49: {1: {'<tree-end>': None}}}, 19477: {1: {'<tree-end>': None}}, 12060: {2998: {1: {'<tree-end>': None}}}, 3: {16196: {989: {288: {1: {'<tree-end>': None}}}}}, 8628: {1: {'<tree-end>': None}}, 955: {122: {1: {'<tree-end>': None}}}, 19099: {1: {'<tree-end>': None}}, 3399: {1: {'<tree-end>': None}}, 19344: {257: {1: {'<tree-end>': None}}}, 12615: {1: {'<tree-end>': None}}, 419: {3389: {4741: {1: {'<tree-end>': None}}}}, 6434: {52: {1: {'<tree-end>': None}}}}\n"
     ]
    }
   ],
   "source": [
    "print(role_tree)\n",
    "if('<tree-end>' in role_tree):\n",
    "    print(\"Wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be1a067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<tree-end>': None}\n",
      "Wrong\n"
     ]
    }
   ],
   "source": [
    "first_tree = role_tree[30837]\n",
    "print(first_tree[1])\n",
    "if '<tree-end>' in first_tree[1]:\n",
    "    print(\"Wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e726fd3",
   "metadata": {},
   "source": [
    "# Transform output to Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b8e19e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bracket(_text):\n",
    "    # replace the special token labels to Formal statement\n",
    "    _text = add_space(_text)\n",
    "    for start in [role_start, type_start]:\n",
    "        _text = _text.replace(start, left_bracket)\n",
    "    for end in [role_end, type_end]:\n",
    "        _text = _text.replace(end, right_bracket)\n",
    "    return _text\n",
    "\n",
    "def add_space(text):\n",
    "    \"\"\"\n",
    "    add space between special token\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_text_list = list()\n",
    "    for item in zip(split_bracket.findall(text), split_bracket.split(text)[1:]):\n",
    "        new_text_list += item\n",
    "    return ' '.join(new_text_list)\n",
    "\n",
    "def get_tree_str(tree):\n",
    "    \"\"\"\n",
    "    get str from event tree\n",
    "    :param tree:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    str_list = list()\n",
    "    for element in tree:\n",
    "        if isinstance(element, str):\n",
    "            str_list += [element]\n",
    "    return ' '.join(str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "39af0a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_0> <extra_id_0> ORG-AFF <extra_id_0> troops <extra_id_0> US <extra_id_1> <extra_id_1> <extra_id_0> Minister Niu Yes No <extra_id_0> British Yellow <extra_id_1> <extra_id_1> <extra_id_0> President <extra_id_0> US <extra_id_1> <extra_id_1> <extra_id_0> officials <extra_id_0> military <extra_id_1> <extra_id_1> <extra_id_1> <extra_id_0> PHYS <extra_id_0> troops <extra_id_0> Iraq <extra_id_1> <extra_id_1> <extra_id_1> <extra_id_0> PART-WHOLE <extra_id_0> military <extra_id_0> US <extra_id_1> <extra_id_1> <extra_id_1> <extra_id_1>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "left_bracket = '【'\n",
    "right_bracket = '】'\n",
    "text = \"<extra_id_0> <extra_id_0> ORG-AFF <extra_id_0> troops <extra_id_0> US <extra_id_1> <extra_id_1> <extra_id_0> Minister Niu Yes No <extra_id_0> British Yellow <extra_id_1> <extra_id_1> <extra_id_0> President <extra_id_0> US <extra_id_1> <extra_id_1> <extra_id_0> officials <extra_id_0> military <extra_id_1> <extra_id_1> <extra_id_1> <extra_id_0> PHYS <extra_id_0> troops <extra_id_0> Iraq <extra_id_1> <extra_id_1> <extra_id_1> <extra_id_0> PART-WHOLE <extra_id_0> military <extra_id_0> US <extra_id_1> <extra_id_1> <extra_id_1> <extra_id_1>\"\n",
    "from nltk.tree import ParentedTree\n",
    "brackets = left_bracket + right_bracket\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8113a5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_0>   <extra_id_0>  ORG-AFF  <extra_id_0>  troops  <extra_id_0>  US  <extra_id_1>   <extra_id_1>   <extra_id_0>  Minister Niu Yes No  <extra_id_0>  British Yellow  <extra_id_1>   <extra_id_1>   <extra_id_0>  President  <extra_id_0>  US  <extra_id_1>   <extra_id_1>   <extra_id_0>  officials  <extra_id_0>  military  <extra_id_1>   <extra_id_1>   <extra_id_1>   <extra_id_0>  PHYS  <extra_id_0>  troops  <extra_id_0>  Iraq  <extra_id_1>   <extra_id_1>   <extra_id_1>   <extra_id_0>  PART-WHOLE  <extra_id_0>  military  <extra_id_0>  US  <extra_id_1>   <extra_id_1>   <extra_id_1>   <extra_id_1> \n"
     ]
    }
   ],
   "source": [
    "split_bracket = re.compile(r\"<extra_id_\\d>\")\n",
    "new_text_list = list()\n",
    "for item in zip(split_bracket.findall(text), split_bracket.split(text)[1:]):\n",
    "    new_text_list += item\n",
    "print(' '.join(new_text_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a4b2eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【   【  ORG-AFF  【  troops  【  US  】   】   【  Minister Niu Yes No  【  British Yellow  】   】   【  President  【  US  】   】   【  officials  【  military  】   】   】   【  PHYS  【  troops  【  Iraq  】   】   】   【  PART-WHOLE  【  military  【  US  】   】   】   】 \n"
     ]
    }
   ],
   "source": [
    "new_text = convert_bracket(text)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7fba06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_tree = ParentedTree.fromstring(new_text, brackets=brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "23f5c276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "  (ORG-AFF\n",
      "    (troops (US ))\n",
      "    (Minister Niu Yes No (British Yellow))\n",
      "    (President (US ))\n",
      "    (officials (military )))\n",
      "  (PHYS (troops (Iraq )))\n",
      "  (PART-WHOLE (military (US ))))\n"
     ]
    }
   ],
   "source": [
    "print(gold_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9a82b518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG-AFF\n",
      "troops \n",
      "US \n",
      "Minister Niu Yes No\n",
      "British Yellow\n",
      "President \n",
      "US \n",
      "officials \n",
      "military \n",
      "PHYS\n",
      "troops \n",
      "Iraq \n",
      "PART-WHOLE\n",
      "military \n",
      "US \n"
     ]
    }
   ],
   "source": [
    "str_list = list()\n",
    "for relation_tree in gold_tree:\n",
    "    print(relation_tree.label())\n",
    "    for role_tree in relation_tree:\n",
    "        a = get_tree_str(role_tree)\n",
    "        print(role_tree.label()+ ' ' +a)\n",
    "        print(role_tree[-1].label() + ' '+get_tree_str(role_tree[-1]))\n",
    "#         print(role_tree[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ee926d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kind = ' '.join(str_list)\n",
    "print(kind)\n",
    "new_kind = ' '.join(kind.split(' ')[1:])\n",
    "print(new_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aa2f458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOC', 'ORG', 'VEH', 'FAC', 'PER', 'WEA', 'GPE', ' ']\n"
     ]
    }
   ],
   "source": [
    "b = []\n",
    "a = [\"LOC\", \"ORG\", \"VEH\", \"FAC\", \"PER\", \"WEA\", \"GPE\"]\n",
    "a.append(\" \")\n",
    "b =a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61d1cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOC', 'ORG', 'VEH', 'FAC', 'PER', 'WEA', 'GPE', ' ', 1]\n"
     ]
    }
   ],
   "source": [
    "c = b+[1]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24f17de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1cb04930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, '']\n"
     ]
    }
   ],
   "source": [
    "a = tokenizer.decode(3)\n",
    "b = [1,2,3]\n",
    "b.append(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8a39edb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1548, 3, 6, 38, 17952, 3859, 3292, 3457, 3, 6, 79, 33, 6326, 53, 3, 9, 775, 682, 13, 4719, 3, 6, 652, 5413, 13, 8, 7749, 9534, 45, 8, 10101, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "c = [0,1]\n",
    "print(c+a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "19a5e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well  , as coalition forces push north  , they are encounter ing  a unique problem of combat  , getting rid of the weapons captured from the enemy  .\n"
     ]
    }
   ],
   "source": [
    "a =  [1548, 3, 6, 38, 17952, 3859, 3292, 3457, 3, 6, 79, 33, 6326, 53, 3, 9, 775, 682, 13, 4719, 3, 6, 652, 5413, 13, 8, 7749, 9534, 45, 8, 10101, 3, 5]\n",
    "b = list()\n",
    "for x in a:\n",
    "    b.append(tokenizer.decode(x))\n",
    "print(' '.join(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6da82dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well', '', ',', 'as', 'coalition', 'forces', 'push', 'north', '', ',', 'they', 'are', 'encounter', 'ing', '', 'a', 'unique', 'problem', 'of', 'combat', '', ',', 'getting', 'rid', 'of', 'the', 'weapons', 'captured', 'from', 'the', 'enemy', '', '.']\n",
      "Well  , as coalition forces push north  , they are encounter ing  a unique problem of combat  , getting rid of the weapons captured from the enemy  .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "labels = torch.tensor(a)\n",
    "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "print(decoded_labels)\n",
    "print(' '.join(decoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7a662991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e6268beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, -1), (7, -1), (10, -2), (12, -2)]\n"
     ]
    }
   ],
   "source": [
    "special_token_set = {-1,-2}\n",
    "tgt_generated = [1,2,3,-1,2,3,2,-1,4,3,-2,4,-2]\n",
    "special_index_token = list(filter(lambda x: x[1] in special_token_set, list(enumerate(tgt_generated))))\n",
    "print(special_index_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bbc632aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><extra_id_0><extra_id_0> PART-WHOLE<extra_id_0> Vivendi Universal<extra_id_0> media group<extra_id_0></s>\n"
     ]
    }
   ],
   "source": [
    "Target = [0, 32099, 32099, 4674, 517, 18, 188, 9089, 32099, 7471, 32099, 13143, 32099, 1]\n",
    "Target = [0, 32099, 32099, 3, 19846, 18, 518, 6299, 3765, 32099, 3, 19600, 989, 23, 12489, 32099, 783, 563, 32099, 1]\n",
    "text = tokenizer.decode(Target)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "34d7e224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The war in Iraq could last months and require considerable US military reinforcements to assure a victory, the Washington Post reported Thursday, quoting senior US military officials.\n"
     ]
    }
   ],
   "source": [
    "Source = [37, 615, 16, 7457, 228, 336, 767, 11, 1457, 10710, 837, 2716, 28050, 7, 12, 7992, 3, 9, 6224, 3, 6, 8, 2386, 1844, 2196, 2721, 3, 6, 3, 8270, 53, 2991, 837, 2716, 4298, 3, 5]\n",
    "text2 = tokenizer.decode(Source)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "37157c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "add_special_tokens\n",
    "print(tokenizer.sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1d8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
